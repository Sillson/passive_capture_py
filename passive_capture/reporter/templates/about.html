{% extends "base_template.html" %}

{% block content %}
<container>
  <div class='col-md-8'>
    <h1>passive capture development notes </h1>
      <p>Below is a catalog of my thought process as I sprinted through the development of this lightweight GeoDjango application. Although far from perfect, I hope to deliver a crude tool at the end of this sprint to enable time-series analysis and visualization to fish passage tracking efforts on the Columbia River. It's been a bunch of fun working with a new platform, and experimenting with statistical/GIS packages in python I may have not had the chance to use otherwise. </p>
    <h2>objectives</h2>
      <p><ul><li><strong>To provide a tool which enables users to create custom time-series forecasts and visualizations for anadramous fish returns along the Columbia River.</strong></li></ul></p>
      <p><ul><li><strong>Compare the efficacy of Facebook's open source time series statistical package versus traditional ARIMA methods</strong></li></ul></p>
    <h2>technologies utilized</h2>
      <ul>
        <li>Python 3</li>
        <li>GeoDjango</li>
        <li>QGIS 3</li>
        <li>BigQuery</li>
        <li>PostGIS</li>
        <li>Leaflet/Folium</li>
      </ul>
    <h2>setup</h2>
      <p>Below are my notes taken during the devleopment of this application</p>
      <p>I've taken a crack at fish passage modeling before, <em>(call it a hobby)</em>, and felt that using the US Army Core of Engineers (USACE) fish passage data for the Columbia River would be the most familiar and applicable for this project. The Fish Passage Center (fpc.org) provides a great interface to collect this data, and I've developed APIs in both Ruby and R to scrape this data.</p>
      <p>I wanted to work with a static set of data initially, as it would require less API overhead than to keep and API up and running to ETL the FPC data on a schedule</p>
      <p>First Steps:
        <ul>
          <li>Downlaod a fresh set of passage / flow data for all dam sites ranging 2010 - 2018</li>
          <li>Create a job to load them into BigQuery</li>
          <li>Normalize and extract the data to CSV</li>
        </ul>
      </p>
      <p><strong>Example Query to normalize the FPC Data Into BigQuery</strong></p>
      <p>
        <code>
            SELECT 
              PARSE_DATE("%m/%d/%Y", date) AS count_date,
              dam,
              ChinookAdult AS chinook_adult,
              ChinookJack AS chinook_jack,
              CohoAdult AS coho_adult,
              CohoJack AS coho_jack,
              Steelhead AS steelhead,
              WildSteelhead AS wild_steelhead,
              Sockeye AS sockeye,
              Pink AS pink,
              Chum AS chum,
              Lamprey AS lamprey,
              Shad As shad
            FROM `passive-capture.passage_data.daily_counts`
            ORDER BY count_date;
        </code>
      </p>
      <p>Perhaps the most interesting aspect of Facebook's open source 'prophet' time-series forecasting package is the ability to incorporate additional regressors into model -- something that has traditionally been impossible. It also comes with options to toggle 'seasonality detection', define custom seasonality trends, or add 'holidays'. Holidays are periods of time the model may expect additonal variance in trend, which seemed appropriate for runs of fish -- which occur seasonally.</p>
      <p>For autoregessors, I hope to examine the impact of current and historical precipitation and dam flows as impacts to the model. Precipiation in both rain and snow play a driving force in fry and parr survival rates, as well as the timing in which salmon species will either 'hold', or push on with their migration. Flows through the dams as well play a critical part in determining parr/smolt predation and survival rates.</p>
      <p>Autogressors
        <ul>
          <li>Current year/month precipitation totals</li>
          <li>Current year/month flows from Dam being modeled</li>
          <li>Current water-year snow/water equivalent standard deviation</li>
          <li>Historical (3yr) year/month precipitation totals</li>
          <li>Historical (3yr) year/month flows from Dam being modeled</li>
          <li>Historical (3yr) water-year snow/water equivalent standard deviation</li>
        </ul>
      </p>


  </div>
</container>


## Aggregate weather data from NOAA: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt


## Example for snow data grouped by year month
```
SELECT
  CONCAT(
    CAST(EXTRACT(YEAR FROM date) AS STRING), 
    CAST(EXTRACT(MONTH FROM date) AS STRING)
  ) AS year_month,
  stations.name,
  stations.latitude,
  stations.longitude,
  weather.id,
  weather.element,
  SUM(weather.value) AS snow_total,
  AVG(DISTINCT weather.value) AS snow_average,
  MIN(weather.value) AS snow_min,
  MAX(weather.value) AS snow_max
FROM
  `bigquery-public-data.ghcn_d.ghcnd_stations` AS stations
  JOIN `bigquery-public-data.ghcn_d.ghcnd_201*` AS weather
  ON weather.id = stations.id
WHERE
  stations.latitude > 43
  AND stations.latitude < 49
  AND stations.longitude > -123
  AND stations.longitude < -120
  AND element = 'SNWD'
GROUP BY
  year_month,
  stations.name,
  stations.latitude,
  stations.longitude,
  weather.id,
  weather.element
ORDER BY
year_month
```

## Conversation with Martin


Use either ARIMA or Prophet for time series analysis on each time series data. Then use Multivariate Regression Analysis

why to take the log of the counts:

The variances in any given samples may differ significantly. 

In log-log regression model it is the interpretation of estimated parameter, say Î±i as the elasticity of Y(t) on Xi(t)

In error-correction models we have an empirically stronger assumption that proportions are more stable (stationary) than the absolute differences.

It is easy to aggregate the log-returns over time.

If you still want a statistical criterion for when to do log transformation a simple solution would be any test for heteroscedasticity. In the case of increasing variance I would recommend Goldfeld-Quandt Test or similar to it. In R it is located in library(lmtest) and is denoted by gqtest(y~1) function. Simply regress on intercept term if you don't have any regression model, y is your dependent variable. 


## Prophet

capture time series data and log of one species at a time
replace nulls with 0's
replace -inf with 0

## Add holidays to Prophet data

http://www.fpc.org/documents/metadata/FPC_Adult_Metadata.html

These are the times we consider 'bonus'

## QGIS

- QGIS 3.0 has some issues it seems
- Create shapefile with inverse polygon of the basin
- 
{% endblock %}